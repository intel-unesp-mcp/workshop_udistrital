{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "import datetime\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import keras.optimizers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import lib.data_loader as dl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this exercise is to build and train a Multilayer perceptron NN to recognize hand-written digits, i.e. from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will be using for this session is called MNIST (http://yann.lecun.com/exdb/mnist/). This is a well known example:\n",
    "\n",
    ">The MNIST database was constructed from NIST's Special Database 3 and Special Database 1 which contain binary images of handwritten digits. NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.\n",
    "\n",
    "The MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers.\n",
    "It is important noting that the digits have been size-normalized and centered in a fixed-size image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read our dataset and draw a sample from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = dl.load_mnist_data_for_mlp() # load the dataset\n",
    "print(\"The shape of the X training set: {}\".format(x_train.shape))\n",
    "print(\"The shape of the Y training set: {}\".format(y_train.shape))\n",
    "print(\"The shape of the X test set: {}\".format(x_test.shape))\n",
    "print(\"The shape of the Y test set: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use an auxiliary function to plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indexes = np.random.randint(len(x_train), size=5)\n",
    "print(\"The selected random indexes: {}\".format(random_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index in random_indexes:\n",
    "    label = y_train[index]\n",
    "    digit = x_train[index]\n",
    "    dl.plot_digit(digit, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a very simple NN with a single fullly connected hidden layer. Our model is sequential, as we don't have feedback in it. First we instatiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the layers to it. For the first layer we specify that it is a fully connected layer by instantiating a `Dense` object, then  need to specify the `input_shape` parameter, that we get from our dataset. After that, we specify a Rectified Linear Unit as its activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(2, activation='relu', input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then specify an output layer with a `softmax` activation function. This function maps an array into a probability distribution with 1 as the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check our model using the `summary()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dat we need to compile our model, by setting which optimizer it will use and what are the metrics that we will use for fitting the model.  \n",
    "A list of optimizers can be found <a href=\"https://keras.io/optimizers/\">here</a>.  \n",
    "A list of loss functions can be found <a href=\"https://keras.io/losses/\">here</a>.  \n",
    "A list of metrics can be found <a href=\"https://keras.io/metrics/\">here</a>.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=\"SGD\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our model. We have to specify a few parameters first.  \n",
    "We have to specify the number of `epochs`, i.e., the number of forward/backpropagation cycles; the `batch_size`, i.e., the number of samples used to train at a time - this parameter optimizes the memory/cache usage. We can also specify a `verbose` parameter to have some feedback while we are training our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is training time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = datetime.datetime.now()\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1)\n",
    "print('Training time: %s' % (datetime.datetime.now() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Available metrics are: \\n{}\".format(model.metrics_names))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again draw a random sample from our test dataset and see how good our predictor is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_draw = np.random.randint(len(x_test), size=5)\n",
    "print(\"Randomly selected test observations index: {}\".format(random_draw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in random_draw:\n",
    "    predicted_value = model.predict_classes(x=x_test[[i]].reshape(1,784), verbose=False)\n",
    "    print(\"Value Predicted: {}\".format(predicted_value))\n",
    "    print(\"Plot: \")\n",
    "    dl.plot_digit(x_test[i], y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to get our hands dirty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a dummy classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy we are getting so far is really bad. How about building one of the dummest classifiers there is: throwing a 10 face die? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Improving the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks are known to behave better on normalized datasets. By knowing that there only 256 (from 0 to 255) possible values for the gray scale. How about normalize the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Changing the NN topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try tweaking the NN topology a bit. Make some small changes in the model:\n",
    "1. Increase the number of neurons at the hidden layer to `128`.\n",
    "1. Add a new 128-neuron `Dense` layer.\n",
    "1. Increase the number of epochs to 10, then 20.\n",
    "1. Add new `Dropout` layers in between the hidden layers (20% is a good start for a dropout).\n",
    "1. Try to use the `categorical_crossentropy` loss function.\n",
    "1. Try to use the `RMSProp` optimizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
